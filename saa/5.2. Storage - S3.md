# Amazon S3

- Key / value store for objects
    - Object => You rewrite entire object
    - Block store => You can just send changes to change the blob, can update incrementally as data is split into chunks.
- ***Objects*** (files) are stored in ***buckets*** (directories)
- **Buckets**
    - `https://s3-<region>.amazonaws.com/<bucket>/<object>`
    - `https://<bucket>.s3.amazonaws.com/<object>` (virtual host style addressing)
      - üìù The website URL will be => bucket name + `s3-website` + region + `amazonaws.com`.
    - `<bucket>.s3-website-<AWS-region>.amazonaws.com`
    - `<bucket>.s3-website.<AWS-region>.amazonaws.com`
  - You can set default encryption
    - Options are: *None*, *AES-256* (SSE-S3) and *AWS-KMS* (SSE-KMS)
  - ‚ùó 100 S3 buckets per account -> soft limit
  - **Version Control**
    - Can enable versioning at bucket level
      - ‚ùóüìù Once enabled, it cannot be disabled but just suspended.
    - Same key overwrite will increment the version
        - Protection against **accidental deletion** i.e. unintended deletes.
        - Deleting files does not really delete versioned files
        - It only puts **Delete marker** and files can still be downloaded.
        - Deleting a delete marker -> Restores the object & adds a new delete marker.
          - ‚ùó Only the owner of a bucket can permanently remove a delete marker e.g. delete a version.
    - üìùAny file that is not versioned prior to enabling versioning will have the version `null`.
- **Objects**
  - Have a **key** which is FULL path
    - e.g. `<my_bucket>/my_file.txt` or `<my_bucket>/my_folder1_another_folder/my_file.txt`
    - ‚ùó The are no concept of directories within buckets just key names with slashes.
  - ***Object values*** are the contents of the file
    - ‚ùó Max size is 5 TB, min size is 0 bytes (touched)
    - ‚ùó If file is more than 5 GB, you need to use multi part upload
    - ***Metadata*** is list of text key/value pairs.
        - ‚ùó Cannot be modified after creation
  - **Logging**
  - Object-level logging
  - Can enable CloudWatch request metrics: e.g. total number of HTTP GET/PUT, 4XX etc.
  - Can enable CloudTrail for auditing API calls and call stacks (not CloudWatch).
- üìù**S3 Consistency Model**
  - Read after write consistency for PUTS of **new objects**
    - E.g. `PUT 200` -> `GET 200`
    - ‚ùó Eventually consistent if you try to `GET` before to see if the object existed
  - Eventual consistency for `DELETE`s and `PUTS` of **existing objects**
    - Read after updating object -> Might get an older version
      - E.g. `PUT 200` -> `PUT 200` -> `GET 200` (might be older version)
    - Delete -> Can retrieve it for a short time
      - E.g. `DELETE 200` -> `GET 200`
- **Make an object publicly accessible**
  - On object level
    - Set a predefined grant (also known as a canned ACL) for a bucket in Amazon S3.
    - Update the object's ACL -> Ensure your object is set to be publicly accessible.
  - On bucket level
    - Use a bucket policy that grants public read access to a specific object tag/prefix
  - S3 data transfer with any AWS services in that same region is free (still pay for requests)
  - **Requester pays**
    - Requester pays for requests + storage in bucket instead of bucket owner.
- **Lifecycle Policies**
  - Set of rules to automating moving data between different tiers, to save storage cost.
  - **Transition actions**
    - When objects are transitioned to another storage class
    - ‚ùó Objects must be stored at least 30 days in standard for standard to => Standard IA or OneZone IA
  - **Expiration actions**
    - Expire & delete an object after a certain time period.
  - You can combine transition & expiration actions.
  - You can configure it to
    - Clean-up expired object delete markers
    - Affect "Current version" and & or "Previous version"
- **Multipart Upload**
  - Use if file is more than >100MB (required for >=5GB)
  - Allows uploading parts concurrently
  - All storage that any parts the aborted multipart upload consumed is freed & deleted.
    - S3 supports a bucket lifecycle rule to abort multipart upload that don't complete within a specified number of days.
- **Event Notifications**
  - **Events**: Object created, object removed, object restored, object lost (RSS)
  - **Destinations**: SNS, SQS and Lambda

## S3 Storage Classes

- Storage class is set based on object
- **S3 Storage Tiers**

    | Storage | Description | Use-cases | Retrieval latency | Cost |
    | ------- | ----------- | --------- | ----------------- | ---- |
    | **Standard** | General purpose | Big Data analytics, mobile & gaming applications, content distribution... | ms | $$$$$$ |
    | **Standard IA** | Infrequent access | For data that's less frequently accessed but requires rapid access e.g. you'll get a file directly every month. | ms | $$$$$ |
    | **One Zone-Infrequent Access** | Low latency & high throughput performance | Storing secondary backup copies of on-premise data or storing data you can recreate over time | ms | $$$$ |
    | **Reduced Redundancy Storage (RRS)** | Deprecated, don't use it, use one zone-infrequent access | Noncritical, reproducible data at lower levels of redundancy than Amazon S3's standard storage | ms | $$$ |
    | **Intelligent Tiering** | ML moves objects between *IA* and *Standard* tiers based on changing access patterns | Unpredictable workloads, Changing access patterns, Lack of experience with storage optimization | ms | *(small)* Monitoring + auto-tiering fee |
    | **Glacier** | Low cost cold storage. | DR Back-ups, Media Asset Archival | Expedited: 1-5 min, Standard: 3-5 hr, Bulk: 5 -12 hr | $$ + retrieval fee |
    | **Glacier Deep Archive** | Lowest cost coldest storage |  Magnetic tapes / VTL / Regulatory archiving  | 12-48 hours | $ + retrieval fee |
- **S3 Storage Tiers Reliability**

  | | Standard | Reduced Redundancy Storage | Standard Infrequent Access | One - Infrequent Access | S3 Intelligent Tiering | Glacier |
  |--|--|--|--|--|--|--|
  | **Durability** | 99.999999999% | 99.99% | 99.999999999% | 99.999999999% | 99.999999999% |  99.999999999% |
  | **Availability** | 99.99% | 99.99% | 99.99% | 99.5% | 99.90% | No SLA |
  | **AZ** | ‚â•3 | ‚â•2 | ‚â•3 | 1 | ‚â•3 | ‚â•3 |
  | **Concurrent facility fault tolerance** | 2 | 1 | 2 | 0 | 1 | 1 |

### Glacier

- Glacier classes moves objects to **Amazon Glacier** which is part of S3.
- **Concepts**
  - **Archive**: store data in (one or more files) with unique archive ID.
    - üìùThe content of the archive is immutable, meaning that after an archive is created it cannot be updated.
  - **Vault**: collection of archives where you upload data to.
- **Retrieval**
  - ‚ùó S3 object stored through Glacier can only be retrieved with S3 APIs as it'll map object path to archive id.
  - **Options**

    | Attribute | Expedited | Standard | Bulk |
    | --------- | --------- | -------- | ---- |
    | Data |  Subset of archives | Any archive | Large amounts |
    | Time | 1 - 5 minutes | 3 - 5 hours | 5 -12 hours |
    | Pricing | $$$$ | $$ | $ |
    - **Expedited**: Access subset of archives in 1-5 minutes.
      - **On-demand**: Accepted except for rare high load situations.
      - **Provisioned**: Buy capacity to ensure that your retrieval capacity is available.
    - **Standard**: Access any archive in 3-5 hours.
    - **Bulk**: Access large amounts in 5-12 hours
  - ‚ùó Deep Archive has only one option for retrieval time around 12-48 hours.
- ‚ùó You cannot upload directly from console.

## Availability

- **Cross region replication**
  - Copying is asynchronous replication
  - On bucket level (entire bucket) or object level (based on prefix/tags)
  - ‚ùóüìù Must enable versioning (source and destination)
    - CCR is built on top of versioning functionality.
    - ü§ó As the replication is asynchronous, it needs 'non-changing' copy of data during replication, and versioning makes it easy => File can still be modified (upload, delete, etc.) while replication is in progress.
  - Buckets must be in different AWS regions & can be in different accounts
  - Requires IAM permissions to S3
  - Delete markers & deleting individual versions / delete markers are not replicated.
  - **Use cases**: compliance, lower latency access, replication across accounts
  - Flow:
      1. Create a bucket, and one more new one as replica
      2. Original bucket -> Management -> Replication -> Add rule
         - Apply to *entire bucket* or *"prefix or tags"*
      3. Choose if objects encrypted with AWS KMS will also be encrypted
         - Other account must also have access to AWS KMS as well
      4. For replicated objects you can change
         - Storage class
         - Ownership to destination bucket owner
      5. Set IAM rule
         - Role gets created automatically that gives replication permissions to the original bucket
    - ‚ùó Does not replicate:
      - Existing files are not replicated, just new files but with same metadata and ACLs.
      - Replicas of other objects, lifecycle actions and configurations, objects without bucket owner permissions, SSE-C & SSE-KMS encrypted object (can be configured to replicate)...

## S3 Websites

- S3 can host static websites and have them accessible on the www.
  - üìù The website URL will be => bucket name + `s3-website` + region + `amazonaws.com`.
    - `<bucket-name>.s3-website-<AWS-region>.amazonaws.com`
    - `<bucket-name>.s3-website.<AWS-region>.amazonaws.com`
- In bucket -> Properties -> Static website hosting
  - **S3 CORS** (CORS=Cross Origin Resource Sharing)
    - üìù If you request data from another S3 bucket, you need to enable CORS
    - Allows you to limit the number of websites that can request your files in S3 and this way limit costs.
    - E.g. `bucket1` has image in `bucket2`. `bucket2` must then has CORS headers for `bucket1`.
- **Prerequisites**
  - ‚ùó The bucket must have the same name as your domain or subdomain.
  - *(Optionally)* Use CloudFront for SSL/TLS
  - A registered domain name.
  - Route 53 as DNS service for the domain to configure DNS.
